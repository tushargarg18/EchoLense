# EchoLens
A lens that returns (echoes) a description

## Idea
Capture real time images to generate accurate captions and then convert text to speech to guide visually impared people.

## Major Areas:
- Architecture
- Vision Encoder
- Text Decoder
- Text to Speech
- Additional features and BOT

[TG - 4/24] - Some research papers that we can refer:
1. https://ieeexplore.ieee.org/abstract/document/10890285
2. https://arxiv.org/abs/1411.4555 -- 2015
3. https://arxiv.org/abs/1502.03044 -- 2016
4. https://aclanthology.org/P18-1238/ -- 2018
5. https://arxiv.org/abs/2201.12086Â --Â 2022

Possible Approach:

## ðŸ§  Smart Project Structure for 4 Team Members

### ðŸŽ¯ Goal:
- Everyone understands core concepts.
- You explore **multiple approaches**.
- You build toward a complete, real-world system.

---

## ðŸ”§ **Phase 1: Diverse Core Implementation**

| Member | Task | Learning Focus | Output |
|--------|------|----------------|--------|
| ðŸ‘©â€ðŸ’» Member A | CNN-based Image Encoder | Learn how ConvNets extract spatial features | Feature extractor module |
| ðŸ‘¨â€ðŸ’» Member B | ViT-based Encoder | Explore patch embeddings and self-attention | ViT-based encoder |
| ðŸ‘©â€ðŸ’» Member C | LSTM/GRU Caption Decoder | Understand sequence generation and training loop | Decoder with attention |
| ðŸ‘¨â€ðŸ’» Member D | Transformer Decoder | Learn positional encoding and multi-head attention | Transformer-based decoder |

> Later, you can **compare BLEU/CIDEr scores** and see which pipeline generalizes better!

---

## ðŸ§ª Phase 2: Evaluation and Ensemble

- Define **shared dataset + evaluation notebook**
- Compare models on same inputs
- Optionally, ensemble or fuse approaches (e.g., late fusion of outputs)

---

## ðŸš€ Phase 3: Modular Extensions

Once you have a strong baseline model:

| Module | Owner (Can rotate roles later) | Description |
|--------|-------------------------------|-------------|
| ðŸŽ¤ Text-to-Speech (TTS) | Member A | Convert captions to speech (pyttsx3, gTTS) |
| ðŸŽ¥ Live Feed + Caption Overlay | Member B | Capture from webcam and display captions |
| ðŸ¤– Robot Integration | Member C | Use Raspberry Pi/Arduino for mobility |
| ðŸ§  Fine-Tuning/Transfer Learning | Member D | Try BLIP-style noisy data filtering or domain-specific tuning |

---

## ðŸ§° Project Setup Recommendations

- Use **GitHub + Branches** (`cnn-encoder`, `vit-encoder`, etc.)
- Organize code in **modular Python packages**
  ```
  project/
  â”œâ”€â”€ data/
  â”œâ”€â”€ encoders/
  â”‚   â”œâ”€â”€ cnn_encoder.py
  â”‚   â””â”€â”€ vit_encoder.py
  â”œâ”€â”€ decoders/
  â”œâ”€â”€ utils/
  â”œâ”€â”€ train/
  â”œâ”€â”€ evaluate/
  â”œâ”€â”€ live_demo/
  â””â”€â”€ main.py
  ```
- Use **Notion, Trello, or GitHub Projects** to track tasks
- Meet weekly for **knowledge sharing + demos**

---

## ðŸ”„ Bonus Ideas for Collaborative Learning

- Everyone writes a mini blog post or summary of what they learned after each phase
- Teach each other during team calls
- Swap modules halfway through the project to get cross-exposure
